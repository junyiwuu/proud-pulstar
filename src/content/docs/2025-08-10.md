---
title: notes 2025-08-10
---

  3. Reduce scheduler calls:
  # Only update schedulers every N steps instead of every step
  if it % 10 == 0:  # Update every 10 iterations instead of every iteration
      scheduler.step()
      scheduler_ks.step()


**can try this, maybe 5 steps? I feel options below are all not really helpful tbh**


â— Yes, reducing scheduler frequency will affect the final result, but the
  impact depends on your specific schedules.

  How it affects results:

  1. Smooths out learning rate changes - Instead of gradual curve, you get
  step-wise changes
  2. Delays critical transitions - Your stage transitions (like at
  first_half = FLAGS.iter * 0.5) might be delayed by up to N steps
  3. Changes convergence behavior - Could be better or worse depending on
  the schedule

  For your specific schedules:

  Your lr_stage1_schedule and lr_stage2_schedule have important transitions
   at first_half. Delaying these by 10 steps might matter.

  Better alternatives:

  Option 1: Smart scheduling - Only update when LR actually changes
  significantly:
  # Update only when LR change is meaningful
  current_lr = scheduler.get_last_lr()[0]
  if it % 10 == 0 or abs(current_lr - prev_lr) > 0.001:
      scheduler.step()
      **(does it really matter?)**

  Option 2: Simpler schedules - Use PyTorch's built-in schedulers:
  # Replace your complex functions with simpler ones
  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
  T_max=FLAGS.iter//2)
  scheduler_ks = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ks,
  T_max=FLAGS.iter//2, eta_min=0.1) 
  **(exactly not what we want)**

  Option 3: Profile first - Measure if scheduling is actually the
  bottleneck:
  import time
  start = time.time()
  scheduler.step()
  scheduler_ks.step()
  print(f"Scheduler time: {time.time() - start:.6f}s")

  I'd recommend Option 2 - your complex schedules might not be providing
  much benefit over simpler alternatives, and simpler = faster + more
  predictable results.


## should reduce hash grid encoding i think