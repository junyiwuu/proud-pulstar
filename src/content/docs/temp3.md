---
title: temp3
---



## personal project:
Be ready to discuss your projects with pride and depth

> Tell me about a project you built â€” what did you do, why, what challenges, what would you change?
So donâ€™t just say â€œI built a game.â€  
Say â€œI solved X problem by doing Y, learned Z.â€

## Leetcode:
- Practice coding problems (HackerRank / LeetCode)
 Strings, arrays, graphs, concurrency basics maybe




## Job knowledge

**Distributed system**
- - latency / bandwidth  tick rate, and synchronization mean  
- What is a distributed system?
- - Brush up on basic distributed systems ideas:
       - clientâ€“server vs peer-to-peer
    - real-time networking basics
- Know a bit about Unreal or multiplayer architectures (even high-level)
 What is latency and why does it matter in multiplayer worlds?
 - Client-server vs peer-to-peer basics
- What issues arise in real-time networked games?  
    (Lag, race conditions, synchronization, packet loss)
> Imagine thousands of players in one world â€” what are some engineering challenges?

- How does networking in games work? (high level)
- API design / building services
- Scalability basics
    âœ… **Real-world engineering questions**


**Developing**
- ~~unit testing~~
- ~~Version control (Git)~~
- ~~CI/CD,~~ ~~IDE mastery, automation~~
- Debugging scenarios
- Building backend services / APIs
- If C++: memory, performance, basics of threads
- game engine integrations (Unreal etc.)
 Learn basics of ECS (Entity Component System) if not already  
âœ… Networking or systems thinking (lightweight)  



**C++**
- C++ fundamentals
- Memory/performance/low-level thinking
- Expect questions like:
- Memory management (stack vs heap)
- Smart pointers
- Move semantics
- Multithreading basics (mutex, race conditions)
- Data-oriented design / cache locality
- Low-latency programming patterns
- Review modern C++ concurrency  

**Graphic**
- Graphics/real-time pipeline awareness
> How would you optimise a real-time simulation loop?
- Check if you can bridge graphics + systems
    - Theyâ€™ll want to see if you understand _why_ virtual worlds need scalable architecture even if you havenâ€™t built it yet
You mentioned Vulkan, so they may ask:
- What is the role of Vulkan in rendering?
- Whatâ€™s the difference between GPU & CPU workloads?
- Why is Vulkan good for high-performance rendering?
- How would you debug a performance bottleneck?
And _maybe_ game engine pipeline concepts:
- Frame pacing
- Draw calls
- Synchronization primitives in graphics
> Why do scalable servers matter even if you're focused on graphics?
> > How can rendering workloads affect networking latency in a virtual world?

> If the rendering side stalls or overloads the GPU/CPU, it can delay simulation and network updates, causing inconsistent player experience. Smooth graphics must complement real-time state syncing.
- A graphics/performance problem you solved
âœ… Project discussion + why Vulkan is useful  







## General question:

> What excites you about virtual worlds?  
> What metaverse tech inspires you?
> - Think through why you want to work in immersive tech
> What do you know about what we do here at msquared


I am excited about this position is because it hits my interests area, I am very interested in graphic area, especially real-time rendering, online-experience and software development. Building virtual world combines these areas.


#### Introduce

Junyi -> start career as CG Generalist -> previously worked in Goodbye Kansas studios -> participated in films, tv series, cinematic game trailer -> whole CG build pipeline -> after years working -> vfx doesn't really match what I want to do -> more interested in tech -> back to uni -> interested in MSquared -> graphic related area, target high efficient large scale system -> love open metaverse idea, lead to web3 -> impact our social life

#### Why you choose not doing visual effects anymore:
There are two main reasons.  
First, through my years in VFX, I realized Iâ€™m naturally drawn to the technical side rather than the artistic one. Iâ€™ve always been curious about how things work behind the scenes â€” for example, I often explored how our procedural assets were built instead of just using the exposed parameters. That curiosity made me want to understand and build things myself.

Second, I wanted a career path where I could apply that technical curiosity more directly. The VFX industry is amazing in creativity, but itâ€™s often structured as project-based service work. Iâ€™m looking for a more sustainable, product-focused environment where I can contribute to building technology that lasts and evolves.

So, moving toward a more technical role feels like a natural progression for me, not a departure.

I appreciate vfx -> but I realized I am naturally more drawn to technical side -> interested in things under the hood instead of pixels -> so career path more technical directed -> vfx is project based, and I am looking for a more product focused 

#### Talk about your project

**Why do this**

real-time vulkan render engine -> personal project, not school -> reason is want to build a software , practice one programming language, also curious about low-level computing, graphic programming -> vulkan manage resource manually 






This project is a real-time rendering engine I developed using Vulkan.  
There are two major reasons I started it is because Iâ€™m really interested in graphics programming and wanted to understand how rendering systems actually work under the hood.  
I also want to improve my software development skills â€” I learned how to structure a scalable, modular systems.

The engine has three main parts: an interactive system for the camera and UI, a precomputation system that generates lighting data, and a rendering system that handles all the drawing logic.  
I implemented physically based rendering with image-based lighting, and I also wrote a small framework around Vulkan to manage all the boilerplate code more easily.

Itâ€™s a long-term project for me, I treat it as a base engine where I can experiment and learn new rendering techniques or research ideas.  
Through this, Iâ€™ve learned a lot about low-level programming, debug and problem solving, and also how to design maintainable software.


I learned both technical and non-technical skills.  
Technically, I gained a deep understanding of how modern graphics APIs and real-time rendering work.  
On the software side, I improved a lot in writing clean, modular code and managing complexity.  
But most importantly, I learned how to keep pushing through steep learning curves â€” Vulkan was very challenging at first, but I really enjoyed figuring things out step by step.



**What is the biggest challenge you met**
- When I started developing my Vulkan rendering engine, I realized that most tutorials only explain how to use the API, not how to design a proper software architecture. A a result, my early versions has no clear module difference, and making the maintenance and expanding very difficult
- I researched open-source vulkan project s on the Reddit and Github, I investigated their module structure, trying to understand why they are designed in this way, how does data flow work. And after rebuild the whole architecture several times, I gradually figure out what I want tna dachieved a modular design that allow further extensions
- What I learned from this experience was not just about architecture itself, but how to analyze othersâ€™ complex code â€” to see what problems they were trying to solve and how they approached them. That helped me clarify my own design goals and figure out a better way to organize my code.


**For debug IBL**
**Challenge:** While implementing image-based lighting, I noticed strange dotted artifacts in reflections. The problem could come from many sources: shader code, surface normals, or incorrect mipmap levels.
**Action:** I compared my output with online examples, rewrote key shader parts with detailed comments, and enabled features incrementally to isolate the issue. I also visualized each mipmap level of the environment map to inspect where the artifact first appeared.
**Result / Learning:** Eventually, I found the problem was caused by a corrupted mipmap layer. This taught me that visualizing intermediate results, even if it increases storage cost, is essential for efficient debugging in graphics programming.






#### What do you know about what we do here at msquared

From what I understand, MSquared focuses on building the infrastructure for the open metaverse. So that creators are more flexible, they can build and own their worlds, not limited by any single company's platform.

MSquared has different product targeting on different use case. 
Your Morpheus platform targeting on agencies and brands, major feature is support thousands concurrent users in the high-end virtual events, which is very impressive. 

Your web world is targeting on the average users, creators, developers, the web world can be launched in the browser without worry about the os platform,  everyone can access it without login. Msquared is building an metaverse ecosystem, so you build the interoperability protocoal, Metaverse Markup language, this gave developers big freemdom  so developers can upload their own asset in the world, and define their own animation that presented in the web world.


I know the parent company, improbable has long history for providing the Massive Multiplayer online service. Previously you developed SpatialOS, and been used for supporting several games, like Mavericks, Worlds Adrift.. 


Imporbable provide the basic operation layer, msquared doing the ecosystem, build API which connect to the improbable's core technology


> This release brings one of MSquaredâ€™s key capabilities - real-time social presence - into the hands of developers. While Morpheus supports thousands of concurrent users in high-end virtual events, Web Worlds now gives creators a way to support hundreds of users in a browser, with no plugins, no installs, and full creative control.


agencies and brands, so they can easily create virtual world that they can used for hosting their events. For example, a virtual world for a car company that everyone can experience their car in the virtual world.
From the users (players) side, the service allows huge amount of online players join in the same virtual world, have interactions with minimum latency.
From client/agency side, they can bring the customised virtual world that fully align their needs in short time.
The company is not same as metaverse that unreal has, or roblox, it is not a online vr chatroom


Previous: spatialOS
Improbable support. Problem is too expensive





Msquared provide the network for the metaverse project The Otherside


#### Why you are interested in join Msquared

I am interested in MSqaured because the company is tacking some of the hardest technical problem of virtual workd. think what MSquared developing is the correct direction of metaverse and has huge potential. I have been following Improbable work for some time, previously improbable building the framework which based on Unity engine, but then experienced things like unity change the agreement and the released game cannot achieve stable cash flow, so they failed and improbable pivot the direction from game to social.

I believe this open metaverse ecosystem will shape how we interact socially, and will be the next phase of internet. I am curious about the technologies that make large-scale, real-time experiences possible â€” distributed systems, networking, and the infrastructure that connects users. I want to participate in building the system, understand how they scale, and learn how theyâ€™re built in practice, how to optimize and how to make things efficient.

I want to contribute to the engineering behind these experiences, learn how these systems operate at scale, and develop the skills needed to build efficient, reliable real-time platforms. This feels like a rare opportunity to learn from people who are genuinely pushing the boundaries of whatâ€™s possible.

What excites me about MSquared is that it feels much closer to that vision than many other so-called â€œmetaverseâ€ projects. MSquared is building an open, distributed ecosystem where creators truly own their worlds and users can interact across connected spaces â€” not just another closed platform or chat room.




My background in graphics programming (Vulkan and rendering pipelines) helps me understand performance constraints on the client side, and Iâ€™m eager to grow into someone who can also design and optimize the systems that make these massive shared worlds run smoothly. I believe this combination fits well with MSquaredâ€™s mission to make immersive, connected experiences accessible to everyone.

I know this distributed system virtual world is very heavy gpu consuming, and be expensive, I am very interested in optimize this task,

I think msquared is slowly reaching it. You have morpheus platform that allow massive multiplayer online come true, Also you have web world and Metaverse markup language that allow users fully customized their own world, from a long term, this will be part of every one social life, I am very interested in this technology. I want to know what kind of difficuties that developer met, 

Your web world is targeting on "creating  your individual world" concept

You have this metaverse markup language is also very cool, it allows user to create anything they want, can put in the customized objects, or create customized components, with this "just a browser", it can be used in many places.


I have the graphic background, I know how does world in the computer been build, but I am getting more and more interested in a more practical question, how virtual world been build and been made it access to many concurrent users. I think making the graphic available to many people and everyone can join together is the next phase of internet, instead of 
From my learning journey in the graphic, I am also developed interests in the high performance system.



Have you done distributed system project, if so , what did you do


**What you can bring to Msquared:**

1. softskill, deteminate
I am a determinant and persistent person, and I have strong self-motivation. First I was interested in AI because I see it impact the vfx industry, so I learn everything from math, because I want to fully understand how does these diffusion network work.
I have always wondering how does rendering engine really work, because it involves how GPU work, how does Vulkan API work, It is a steep learning curve, but I made it, and I have something to show.

2. Study ability
More, I think I have strong study ability, I know how to learn, these are experience from my long time self-learning. First, I am not passive learner. During my first master degree in vfx, because we don't have math lesson, and I was having some feeling that I will need to learn advance math, I spent my summer holiday on learning math, even I didn't have very obvious target back to that time. 
I have my own notebook system, I record the issue that I met and the concept that I am not clear in my notebook system

3. experience, familiar with graphic:

I think after developing my render engine, I am naturally care about the efficiency of the software, I have the awareness of the efficiecy of the software.
Since I have visual effects artist experience and graphic programming experience, I understand what users want, what is computational heavy in the scene, and what are the optimization methods could be. I know that the job probably not optimize the scene directly, because I saw you have job post hiring render engineer, and this probably would be their focus, but I believe understand these could help to build the infrastructure

I know that in the job description, you mentioned about unreal integration, I haven't learn the unreal engine yet, but I learn 3D software very fast, because in the vfx job, since I was a generalist, I need to use whatever that can achieve the result. all 3D software are not too far from each other and following the similar ideology, the major difference would be their goal, so it is not difficult for me to understand them.

for example when I do the concord project, we have a close up scene that need to see the ground crack, we cant use tecture for that since it is too close to the camera, so I have to use zbrush, which is a professional sculpting software, to do it, also high resolution geometry have different workflow compare to the average asset, since it can be too heavy on memory, there are some optimizaiton workflow that I have to follow. I achieve the goal in one week, I use online tutorial as the quick warmup, understanding the major tool and workflow. 



My understanding of graphic is helpful for building the system, rendering could be a heavy thing for many devices, find a way to optimize them is very crucial, my background in visual effects allow me to understand the limitation of rendering, since our asset usually is high details compare to game, I know the line

I always have awareness of how heavy is the scene with just a look, I beleive that could be useful when developing the distributed system.



> This release brings one of MSquaredâ€™s key capabilities - real-time social presence - into the hands of developers. While Morpheus supports thousands of concurrent users in high-end virtual events, Web Worlds now gives creators a way to support hundreds of users in a browser, with no plugins, no installs, and full creative control.
> https://msquared.io/blog/introducing-crowd-support-in-web-worlds





#### Question:

I am wondering roughly how many graduates you are looking to bring on this position.
Also roughly how many people in MSquared, what department you have, what is the company structure.

I am curious about the company's broader vision, do you think MSquared mainly focus on the brands or agency client, or have a plan in the near future to advertie or introduce web world  to more average developers and users?


what does the career path for someone in this role look like in MSquared?
 

From your experience, what kind of qualities or habits do the most successful people on your team usually have â€” what makes them stand out beyond just doing whatâ€™s expected?











What is Agile:
1. breaking down tasks
	> the project break down a list of tasks, such as design user interface, develop product details page
2. Sprints
	> normarly around 2-4 weeks,  each team choose to do certain task. Each team has their own team meeting, figure our how to achieve the task
3.  Daily stand-up meetings
	> no more than 15 minutes, each team member talked about what they have done yesterday, what they are going to do today, what issue they met etc.
4. Review and Retrospective
	> review meeting, each team discuss what they done good during the sprint, what can be adjusted and improve
5.  The next sprint
	> choose the next task and sprint






























   






6. **å®ç°ç»†èŠ‚ (How)**
    
    - ä½ è§£å†³äº†å“ªäº›å…³é”®æŠ€æœ¯é—®é¢˜ï¼Ÿ
        
        - Vulkan setup & synchronizationï¼ˆCommand buffers, Semaphores, Fencesï¼‰
            
        - Descriptor sets & uniform buffer design
            
        - Memory allocation & staging buffer
            
        - Pipeline caching and layout design
            
        - Multithreading or render graph systemï¼ˆå¦‚æœæœ‰ï¼‰
            
7. **éš¾ç‚¹ä¸æŒ‘æˆ˜ (Problem-solving)**
    
    - å“ªäº›åœ°æ–¹æœ€éš¾ï¼Ÿä½ æ˜¯æ€ä¹ˆdebugæˆ–è€…ä¼˜åŒ–çš„ï¼Ÿ
        
    - ä¸¾ä¸€ä¸¤ä¸ªä½ â€œå¡äº†å¾ˆä¹…ç„¶åè§£å†³äº†â€çš„æŠ€æœ¯ç‚¹ã€‚
        
8. **æˆæœ / å±•ç¤º / æ€§èƒ½ / å¯æ‰©å±•æ€§ (Results)**
    
    - å®ç°äº†å“ªäº›æ•ˆæœï¼Ÿ
        
    - æ¸²æŸ“æ€§èƒ½æ€ä¹ˆæ ·ï¼Ÿ
        
    - æœ‰æ²¡æœ‰ modularityã€å¯æ‰©å±•æ€§ã€è·¨å¹³å°æ€§ï¼Ÿ
        
9. **åæ€ä¸æœªæ¥æ”¹è¿› (Whatâ€™s next)**
    
    - ä½ å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ
        
    - å¦‚æœå†åšä¸€æ¬¡ï¼Œä¼šæ€ä¹ˆæ”¹è¿›ï¼Ÿ
        
    - ä¸‹ä¸€æ­¥è®¡åˆ’æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆæ¯”å¦‚åŠ ä¸Š deferred rendering / PBR / compute pipeline / RTï¼‰
        


ä½ å¯ä»¥å°è¯•é€ä¸ªå›ç­”ã€æ•´ç†æ€è·¯ï¼š

1. ä½ ä¸ºä»€ä¹ˆå†³å®šè‡ªå·±åšä¸€ä¸ª Vulkan engineï¼Ÿ
    
2. ä½ çš„å¼•æ“æ•´ä½“æ¶æ„æ˜¯æ€æ ·çš„ï¼Ÿæ¨¡å—åˆ’åˆ†ï¼Ÿ
    
3. ä½ çš„æ¸²æŸ“å¾ªç¯ï¼ˆframe loopï¼‰æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ
    
4. Vulkan çš„åˆå§‹åŒ–æ­¥éª¤æœ‰å“ªäº›ï¼Ÿå“ªäº›æœ€å®¹æ˜“å‡ºé”™ï¼Ÿ
    
5. ä½ æ˜¯æ€ä¹ˆå¤„ç† GPU memory allocation çš„ï¼Ÿç”¨äº† VMA å—ï¼Ÿ
    
6. ä½ çš„ descriptor set æ˜¯æ€ä¹ˆç»„ç»‡çš„ï¼ŸåŠ¨æ€ uniform buffer æ€ä¹ˆç®¡ç†ï¼Ÿ
    
7. ä½ æ˜¯æ€ä¹ˆè°ƒè¯• Vulkan çš„ï¼Ÿ
    
8. ä½ æœ‰æ²¡æœ‰å®ç°æŸç§æ¸²æŸ“æŠ€æœ¯ï¼ˆforward / deferred / PBR / shadow mappingï¼‰ï¼Ÿ
    
9. æ€§èƒ½æ–¹é¢ä½ åšè¿‡ä»€ä¹ˆä¼˜åŒ–ï¼Ÿ
    
10. ä½ æœ€å¤§çš„æŠ€æœ¯éš¾ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯æ€ä¹ˆè§£å†³çš„ï¼Ÿ
    
11. å¦‚æœä½ è¦åœ¨è¿™ä¸ªå¼•æ“ä¸Šå®ç° ray tracing æˆ– compute shaderï¼Œä¼šæ€ä¹ˆè®¾è®¡æ¥å£ï¼Ÿ
    
12. ä½ è§‰å¾— Vulkan ç›¸æ¯” OpenGL æœ€éš¾çš„éƒ¨åˆ†æ˜¯ä»€ä¹ˆï¼Ÿä½ æ˜¯æ€ä¹ˆå…‹æœçš„ï¼Ÿ
    

---


Distributed system:
- computer operate concurrently
- computer fail independently
- computers do not share a global clock




latency when synchronization


Distributed system data flow: 
Object storage -> RabbitMQ -> consumer -> databse\ -> API server
é¦–å…ˆç”¨æˆ·ä¸Šä¼ æ–‡ä»¶ï¼Œå­˜å‚¨åˆ°object storage
 å‘æ¶ˆæ¯ç»™rabbitMQæ˜¯é˜Ÿåˆ—
 consumeræ‹¿åˆ°æ¶ˆæ¯æ‰§è¡Œä»»åŠ¡
 å¤„ç†ç»“æœå†™å…¥æ•°æ®åº“
 apiæœåŠ¡å™¨æä¾›æ¥å£ç»™å‰æ®µè¯»å–ç»“æœ

|ç»„ä»¶|ä½œç”¨|ç±»æ¯”|
|---|---|---|
|**Object Storageï¼ˆå¯¹è±¡å­˜å‚¨ï¼‰**|å­˜æ”¾å¤§æ–‡ä»¶ï¼ˆå›¾ç‰‡ã€è§†é¢‘ã€æ–‡æ¡£ç­‰ï¼‰ã€‚æ¯”å¦‚ AWS S3ã€é˜¿é‡Œ OSSã€MinIOã€‚|äº‘ç¡¬ç›˜/ç½‘ç›˜|
|**RabbitMQï¼ˆæ¶ˆæ¯é˜Ÿåˆ—ï¼‰**|ä¸´æ—¶ä¿å­˜â€œä»»åŠ¡æ¶ˆæ¯â€ï¼Œè®©åå°æ…¢æ…¢å¤„ç†ã€‚é˜²æ­¢ç¬æ—¶é«˜å¹¶å‘å‹å®æœåŠ¡ã€‚|ä»»åŠ¡å¾…åŠç®±|
|**Consumerï¼ˆæ¶ˆè´¹è€…ï¼‰**|åå°è¿›ç¨‹ï¼ŒæŒç»­ç›‘å¬é˜Ÿåˆ—ï¼Œæœ‰æ¶ˆæ¯å°±æ‹¿å‡ºæ¥å¤„ç†ã€‚|æœºå™¨äººå·¥äºº|
|**Databaseï¼ˆæ•°æ®åº“ï¼‰**|å­˜å‚¨ç»“æ„åŒ–ç»“æœï¼ˆæ¯”å¦‚ä»»åŠ¡çŠ¶æ€ã€è¯†åˆ«ç»“æœç­‰ï¼‰ã€‚|ä»“åº“/ç™»è®°è¡¨|
|**API Serverï¼ˆæ¥å£æœåŠ¡å™¨ï¼‰**|æä¾›ç»Ÿä¸€çš„æ¥å£ç»™å‰ç«¯æˆ–å…¶ä»–ç³»ç»Ÿè°ƒç”¨ã€‚|çª—å£/æ¥å¾…å‘˜|
å‡è®¾æ²¡æœ‰è¿™äº›ç»„ä»¶ï¼Œä½ å°±ä¼šï¼š
- ç”¨æˆ·ä¸€ä¸Šä¼ æ–‡ä»¶ â†’ åç«¯ç«‹åˆ»å¤„ç† â†’ æ•°æ®åº“ç«‹åˆ»å†™ç»“æœï¼›
- å¦‚æœä¸Šä¼ é‡å¤ªå¤§ï¼Œåç«¯ç«‹åˆ»â€œçˆ†æ‰â€ã€‚
    

è€Œé€šè¿‡è¿™ç§åˆ†å±‚ç»“æ„ï¼š
- **å¯¹è±¡å­˜å‚¨** å…ˆä¿å­˜æ–‡ä»¶ï¼›
- **æ¶ˆæ¯é˜Ÿåˆ—** ç¼“å†²ä»»åŠ¡ï¼›
- **æ¶ˆè´¹è€…** å¼‚æ­¥å¤„ç†ï¼›
- **æ•°æ®åº“ + API** å†æ…¢æ…¢æš´éœ²ç»“æœï¼›  
    â†’ ç³»ç»Ÿæ—¢æŠ—å‹ï¼Œåˆèƒ½æ°´å¹³æ‰©å±•ã€‚


å‡è®¾ç”¨æˆ·ä¸Šä¼ è§†é¢‘å¹¶å¸Œæœ›è·å–è½¬ç ç»“æœï¼š
1. **API Server** æ”¶åˆ°ä¸Šä¼ è¯·æ±‚ï¼ŒæŠŠè§†é¢‘å­˜è¿› **Object Storage**ï¼›
2. å®ƒå¾€ **RabbitMQ** é‡Œå‘ä¸€æ¡æ¶ˆæ¯ï¼ˆâ€œè¯·å¤„ç†è¿™ä¸ªè§†é¢‘â€ï¼‰ï¼›
3. **Consumer**ï¼ˆåå°è¿›ç¨‹ï¼‰çœ‹åˆ°æ¶ˆæ¯ï¼Œå°±å»ä¸‹è½½æ–‡ä»¶å¤„ç†ï¼›
4. å¤„ç†å®ŒæŠŠç»“æœå†™è¿› **Database**ï¼›
5. ç”¨æˆ·å†æ¬¡è®¿é—® **API Server** â†’ æŸ¥è¯¢æ•°æ®åº“ â†’ æ‹¿åˆ°ç»“æœã€‚

What does scalable mean:
When the load increases, as long as we add more resources( machines, cpu or instances), the overall system performance can scale linearly or nearly linearly.


ä¹Ÿå°±æ˜¯è¯´ï¼š
- ç”¨æˆ·å¤šäº† â†’ ä½ ä¸éœ€è¦é‡å†™ä»£ç ï¼›
- åªè¦å¤šåŠ å‡ å°æœºå™¨ï¼Œç³»ç»Ÿå°±èƒ½æ’‘ä½ã€‚

e

|ç±»å‹|è§£é‡Š|å…¸å‹ä¾‹å­|
|---|---|---|
|**å‚ç›´æ‰©å±•ï¼ˆVertical Scaling / Scale Upï¼‰**|å‡çº§å•æœºé…ç½®ï¼ˆæ›´å¿« CPUã€æ›´å¤§å†…å­˜ï¼‰|ä¹°æ›´å¼ºçš„æœåŠ¡å™¨|
|**æ°´å¹³æ‰©å±•ï¼ˆHorizontal Scaling / Scale Outï¼‰**|å¢åŠ æœºå™¨æ•°é‡ï¼ˆåˆ†å¸ƒå¼éƒ¨ç½²ï¼‰|å¢åŠ èŠ‚ç‚¹ã€å®ä¾‹ã€å®¹å™¨|
















The major reason I do this project is because of I am interested in graphic programming and understand how to build a software. , first I can understand how to bring the data to the screen, second I can learn software development and low level programming. These are two main reason that I do the project.

My actual goal is make it as a long term project, the bottom logic of a vulkan render engine is I can show something on the screen, so it can be diverted into many idea later.
if I interested in some new research algorithm, I can implement it in my engine to use it. If I want to make it 
I feel building it can learn actual skills that can benefit me in the long term, so I choose to do the project.


The major modules are interactive system, rendering system and precompute system. Also there are Vulkan boilerplate like device, swapchain etc.

Interactive system responsible the lifecycle of ImGUI UI and interactive camera, we register the callback of GLFW window about mouse position, click, actions, to compute the corresponding projection and view matrix, so to  correctly updating the camera position.

The precompute system is responsible for pre-generate the data and textures for prefiltered environment map, irradiance map and BRDT LUT and save them on the disk. our input environment map will be compute here, automatically generate mipmaps and been used for computing diffuse and reflection in the fragment shader. The generation happen when the software launches, so during the runtime it just read the data, not generate it.

The rendering system is responsible for the major drawing logic. Here I upload the model and texture, and created the corresponding GPU resources, setup all vulkan needs, such as graphic pipeline, binding descriptors, 


I developed this Vulkan real time rendering engine, because I want to go deep understand the modern graphic api, and how does real-time rendering working, how to write PBR, how to construct a software.
I achieve the physically based rendering, driven by the metallic roughness workflow. Then I use Image-based lighting, I precompute the irradiance map and pre-filtered environment map via compute pipeline, for diffuse and specular shading.

Understanding Vulkan API is a steep learning curve. But it also rewarding, I have a strong aware of the low level manipulation, for example if I want to loading the texture, I need to load the data into CPU, then change the image layout, change the flag to make sure the GPU side can "see" it, then load them into GPU.

I have learnt more via this project, 
Also since Vulkan API require a lot boilerplate, so I have to write a lot of classes to wrap them, and I feel I have practice a lot of OOP programing through this






- **Architecture**
    - How is your engine structured? Do you have:
        - a **render graph**, or a sequence of hardcoded passes?
        - a **resource manager** for buffers, images, descriptor sets?
        - an ECS or scene graph? What data structures?
    - How do you handle **synchronization**: pipeline barriers, image layout transitions, CPUâ€“GPU sync?
        
- **Memory & performance**
    - How do you manage GPU memory? Are you using `VMA` or hand-rolled allocators?
    - How many draw calls can you push at 60 FPS on your target GPU?
    - Did you profile GPU vs CPU? With what tools?
        
- **Correctness & maintainability**
    
    - Any **unit tests**? Regression tests (e.g. golden images)?
        
    - How do you structure your **CMake/build**?
        
    - Are there **debug layers** enabled? Validation error-free?
        
- **C++ level**
    - Which **C++ standard** (17/20)?
    - How do you handle ownership (RAII, smart pointers)?
    - Any use of templates, ranges, constexpr, etc.?


    
- Big gaps for a typical SWE/tech interview:
    
    - Almost **no mention of algorithms / data structures / complexity**.
        
    - No mention of **testing, CI/CD, code quality practices, design patterns**.
        
    - Your work experience bullets are **artist-focused**, not **engineering-focused**.
        
    - Some **typos and template garbage** that will annoy picky reviewers.
        

If I were screening you for a graphics/ML/engineer role, Iâ€™d still be interested, but Iâ€™d have a lot of questions.
 
---

 
2. **Modules list**  
    Currently:
    
    > Computer Programming, Computer Architecture and Networks, Database Systems, Software Engineering, Data Analytics, Security and Authentication. Auditing: Algorithms and Data Structure, Machine Learning.
    
    For a tech interview, Iâ€™d immediately ask:
    
    - Why are **Algorithms and Data Structures** only _audited_? Can you confidently solve:
        
        - time/space complexity questions,
        - common DS (trees, graphs, heaps, tries),
        - standard problems (two-pointer, DP, BFS/DFS, Dijkstra, etc.)?
    - What **projects** did you do in these courses? No mention. Thatâ€™s a missed opportunity.
        Answer: these are auditing courses, I didn't do any project for these courses, but I practices leetcode (?)
    
    
- The snowstorm simulation sounds cool, but:
    - What tools / languages? Houdini-only? Any custom VEX/Python?
    - Any performance constraints, e.g. simulated N particles in T seconds?
    - Did you write any custom solvers or only used node-based setups?
---


This is the strongest part of your CV. This is where I, as a hiring manager, actually get interested â€” or completely lose interest â€” depending on how â€œrealâ€ these are.

### J-Renderer: Interactive Vulkan Engine

> Vulkan rendering engine from scratch in C++ â€¦ PBR, IBL, HDR skybox, ImGui, Arcball camera, dynamic rendering, MSAA, tangent-space normal mapping.

Strong content, but still too â€œfeature-listyâ€ and not â€œengineering-yâ€.

Brutal questions Iâ€™d ask you:

**Git**
What is PR



6. **Architecture**
What is the architecture of the project



- How is your engine structured? Do you have:
        - a **render graph**, or a sequence of hardcoded passes?
        - a **resource manager** for buffers, images, descriptor sets?
        - an ECS or scene graph? What data structures?
            
    - How do you handle **synchronization**: pipeline barriers, image layout transitions, CPUâ€“GPU sync?
        
2. **Memory & performance**
    
    - How do you manage GPU memory? Are you using `VMA` or hand-rolled allocators?
hand rolled allocator
    gpu memory :
    JBuffer: 
	    - create vk buffer info, tell the size, and usage, create the vkbuffer
	    - memory requirement checking, what does this buffer need what requirement (what gpu memory type)


from buffer, understand what gpu memory type
then find memory type index by , findMemoryType, input memory property, physical device, get memory properties from physical device, and see if match with the input memory property

buffer usage: transfer source or transfer destination etc
memory property: host coherent or host visible or device local




**What is MSAA and how to use MSAA here**
multi-sample anti-anallising



**what is difference between use 3 frames in flight and one frame in flight**




**ä¸ºä»€ä¹ˆè¦å¯¹é½ï¼Ÿlayout transitionçš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ**
1. ä¸ºä»€ä¹ˆè¦å¯¹é½ï¼ˆalignmentï¼‰ï¼Ÿ**
å› ä¸º GPU è®¿é—®å†…å­˜æœ‰ç¡¬ä»¶å¯¹é½è¦æ±‚ã€‚
- UBO/SSBO æœ‰æœ€å°å¯¹é½ç²’åº¦ï¼ˆæ¯”å¦‚ 256 bytesï¼‰
- Image row pitch æœ‰å¯¹é½
- Buffer copy ä¹Ÿæœ‰å¯¹é½
å¦‚æœä¸æŒ‰å¯¹é½è¦æ±‚åˆ†é…æˆ–è®¿é—®å†…å­˜ï¼ŒGPU æ— æ³•æ­£ç¡®åŠ è½½æ•°æ®ï¼Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ç”šè‡³ undefined behaviorã€‚
æœ¬è´¨ï¼šç¡¬ä»¶åªèƒ½æŒ‰ç…§ç‰¹å®šå—å¤§å°è¯»å–å†…å­˜ï¼Œæ‰€ä»¥èµ„æºå¿…é¡»æŒ‰å¯¹é½åˆ›å»ºã€‚



How many draw calls can you push at 60 FPS on your target GPU?
        
    - Did you profile GPU vs CPU? With what tools?
        
3. **Correctness & maintainability**
    
    - Any **unit tests**? Regression tests (e.g. golden images)?
        
    - How do you structure your **CMake/build**?
        
    - Are there **debug layers** enabled? Validation error-free?
        
4. **C++ level**
    
    - Which **C++ standard** (17/20)?
        
    - How do you handle ownership (RAII, smart pointers)?
        
    - Any use of templates, ranges, constexpr, etc.?
        

**Resume improvements:**

Right now:

> The engine features Physically Based Shading, Image Based Lighting, HDR skyboxâ€¦

Consider adding _impact_ and _tech detail_:

- â€œ~X lines of modern C++ (C++20) with RAII-based resource management and a minimal render-graph abstraction over Vulkan pipelines.â€
    
- â€œAchieves 120 FPS rendering of scenes with ~Y objects / Z draw calls on [GPU] at 1080p.â€
    
- â€œImplemented GPU resource allocator and descriptor set cache to minimize reallocations.â€
    

### Stable PBR Texture Extraction on Shiny Surfaces with Nvdiffrec

> Modified Nvdiffrecâ€™s material optimization by splitting it into two branches with different learning rates and loss functionsâ€¦

This sounds really interesting, but itâ€™s too vague for a technical reviewer.

Questions:

1. **Math / loss design**
    
    - What exactly are the **two branches** optimizing? Which parameters where?
        
    - What loss functions did you use? (L1, L2, perceptual, regularization?)
        
    - How did you choose different learning rates, and why?
        
2. **Results**
    
    - How did you measure â€œmore stable texture recoveryâ€? PSNR / SSIM / LPIPS? Qualitative only?
        
    - On how many scenes / samples? Any metrics you can quote?
        
3. **Engineering**
    
    - Framework: PyTorch? JAX? Native CUDA?
        use pytorch

    - Did you refactor Nvdiffrec code or hack on top? Any PRs upstream?
        

**Resume improvements:**

Add something like:

- â€œImplemented dual-branch optimization of basecolor/roughness using PyTorch, reducing flickering artifacts on shiny surfaces by ~X% (measured via Y metric) on a dataset of Z scenes.â€
    
- â€œRefactored Nvdiffrecâ€™s material pipeline to modularize loss terms and support experiment configuration via YAML/JSON.â€
    

### Beauty2Albedo (Stable Diffusion)

> Fine-tune the Stable Diffusion model to predict Albedo maps from Beauty renders by modifying the UNet component.

Again, good concept, but no scale/details.

Questions:

1. **Dataset**
    
    - How many image pairs? What resolution?
        
    - How did you generate the dataset? From your own renders or a public dataset?
        
2. **Training**
    
    - Which SD version (1.5, 2.1, SDXL)?
        
    - Training duration, GPU, batch size?
        
    - Any tricks: LR schedule, EMA, mixed precision?
        
3. **Engineering**
    
    - How is the code structured? Module layout? Config system?
        
    - Any evaluation metrics? Baselines you compared to?
        
    - Did you build a small inference UI / CLI?
        

**Resume improvements:**

- Add scale + tools: â€œTrained on ~NK image pairs at 512Ã—512 using PyTorch and xFormers on a single RTX 4090, achieving [metric or qualitative improvement claim].â€
    
- Mention modifications specifically: â€œInjected skip-connection residual blocks in U-Net mid-block to better preserve spatial structure of underlying albedo.â€
    

---

## Work Experience â€“ Goodbye Kansas Studios

Title: **â€œGeneralist Artist / Envrionment Artistâ€**  
â†’ â€œEnvrionmentâ€ is a typo. For a picky engineer, a typo in your title is a big â€œdo you check your own work?â€ flag. Fix this immediately.

Overall issue: This section reads like **artist CV**, not **engineer/problem-solver** CV. For a tech interview, I would ask:

> â€œWhere is the _coding_? Where is the _automation_, _tools_, _scripts_? What did _you_ build that saved the team time or enabled something new?â€

Letâ€™s go bullet by bullet.

 Bullet 1

> Managed the assembly and layout of 3 main/complex 3D environmentsâ€¦ including assets modelling, texturing, look developmentâ€¦

Questions:

- â€œManagedâ€ means what? You led other artists, or you personally did most of the work?
    
- Did you write any **tools/scripts** to help layout, scattering, or asset management? Houdini[[#Bullet 3]] digital assets? Python for USD?
    
- Any **quantifiable** outcome: reduced environment build time by X%, handled scenes with Y million polys?
    

Resume improvement:  
Make it clearer what your engineering-ish contributions were:

- â€œBuilt and maintained 3 hero USD environments for cinematics, using Houdini + USD; created Python tools for asset layout and scatter that reduced manual dressing time by ~30% for a team of 6 artists.â€
    

(Only put the percentage if itâ€™s not made up.)

Bullet 2

> Achieved building high quality CG set extension for 6 shows, utilized on-set data like Lidar scans and HDRIâ€¦

Questions:

- Whatâ€™s â€œhigh qualityâ€? Did supervisors re-use your setups across multiple shows?
    
- Any processing of **Lidar data** using scripts? Automation for alignment, cleaning, decimation?
    
- What tools and data formats (e.g. Alembic, USD, custom)? Any optimization for render times?
    

Resume improvement:

- Add tech and process: â€œProcessed Lidar scans (Xâ€“Y points each) and HDRI data to build physically plausible set extensions in Houdini and USD, optimizing geometry and textures to keep render times under Z min/frame in [renderer].â€
    

Bullet 3

> Closely collaborated with multi-disciplinary teams and supervisors. Participated in 1-2 daily review meetingsâ€¦

This is fine but generic. For SWE, weâ€™d rather see:

- When did you **push back** on unrealistic requests?
    
- Did you **clarify requirements**, or **redefine scope** to fit deadlines?
    
- Any examples where you proposed a technical solution instead of brute-force manual work?
    

Consider compressing this into one sentence and using the space for more technical content.

Bullet 4

> Refined and up-scaled models and textures from external vendors. Efficiently created and updated automated assets for team use, and integrating them into pipelines(USD).

This is the first bullet that hints at **automation**, which is gold for your SWE story.

Questions:

- What do you mean by â€œautomated assetsâ€?
    
    - Houdini digital assets? Python tools that auto-generate things from parameters?
        
    - Templates in USD that auto-wire shading?
        
- What **languages/tools** did you use? Python, VEX, USD API, command line?
    
- Did you reduce some process from hours to minutes? How many artists used your assets?
    

Resume improvement:

Turn this into a clear engineering bullet, e.g.:

- â€œDeveloped Python tools and USD-based procedural assets to standardize vendor geometry and materials, cutting per-shot asset clean-up from ~2 hours to ~20 minutes across N artists.â€
    

Bullet 5

> Reflected promptly to changes from upstream departmentsâ€¦

This sentence is a bit awkward (â€œreflected promptlyâ€ is unusual wording).

Questions:

- Did you just react quickly, or did you build **robust, non-brittle setups** so changes propagated automatically?
    
- Any examples where your pipeline design made a late change easy instead of catastrophic?
    

Resume improvement:

- â€œDesigned non-destructive Houdini/USD workflows so that late upstream changes (animation, layout) propagated automatically to lighting/compositing, avoiding manual rework on N+ shots.â€
    

Projects list & showreel link

Good to have these; just make sure:

- The password for the showreel is not something silly (it currently is â€œ123456â€ â†’ this looks unprofessional; use something neutral or remove password if not needed).
    
- If possible, highlight **one or two projects** where your tech contributions were strongest and be ready to talk in depth about them.
    

---

## Skills

> C++, Python, Linux, Git  
> VFX: Houdini, USD, VEX, Maya, Substance, Nuke, etc.

For a tech interview, this is too shallow. As a picky senior:

Questions:

1. C++
    
    - What level? Can you:
        
        - Explain move semantics, RAII, value vs reference semantics?
            
        - Use smart pointers correctly (unique_ptr/shared_ptr/weak_ptr)?
            
        - Discuss O(â€¦) complexity and memory of your data structures?
            
    - Any familiarity with STL (vector, unordered_map, algorithms), or more advanced parts (ranges, coroutines)?
        
2. Python
    
    - Mostly scripting inside Houdini/Maya/Nuke, or also standalone apps?
        
    - Any use of:
        
        - PyTorch/TensorFlow,
            
        - packaging, virtualenv/poetry,
            
        - testing (pytest), logging?
            
3. Linux
    
    - Daily driver? Comfortable with shell, SSH, basic debugging (strace, gdb) or just â€œI used it at workâ€?
        
4. Git
    
    - Just basic commit/pull/push, or branching strategies, code review, resolving conflicts, bisect?
        
5. Missing but implied skills:
    
    - GLSL/HLSL or shader languages (you do PBR rendering; surely you write shaders).
        
    - CUDA / GPU compute? Any?
        
    - PyTorch for the ML projects.
        
    - Build systems (CMake, Ninja).
        

**Resume improvements:**

Make this more structured and tailored towards engineering roles:

- **Languages:** C++ (C++20, strong), Python, GLSL, VEX
    
- **Core:** Algorithms & Data Structures (self-study + interview prep), OOP, multithreading basics
    
- **Graphics:** Vulkan, PBR shading, IBL, HDR, GPU profiling
    
- **ML:** PyTorch, Stable Diffusion fine-tuning, Nvdiffrec
    
- **Tools:** Linux, Git, CMake, Houdini, USD, Maya, Nuke, etc.
    

(Only list â€œAlgorithms & Data Structuresâ€ if youâ€™ve actually done serious prep.)

---

  
## What Iâ€™d expect you to be asked (and you should prep for)

Here are concrete, adversarial questions Iâ€™d ask in an interview based on your CV:

### On J-Renderer

- Walk me through your **rendering pipeline** from CPU scene representation to pixels on screen.
    
- How do you handle **swapchain recreation** on window resize?
    
- How do you manage **descriptor sets** for materials? What happens when you have thousands of objects?
    
- Show me a bug you hit related to Vulkan synchronization and how you debugged it.
    
- How would you refactor your engine if you wanted to add **deferred rendering** or **shadow mapping**?
    

### On Nvdiffrec / Beauty2Albedo

- How did you **split the optimization branches** in Nvdiffrec? What parameters did each branch optimize?
    
- What was your **training loop** for Beauty2Albedo? Show pseudo-code.
I ran for 8k iterations, 
    
- How do you avoid **overfitting** when fine-tuning Stable Diffusion on a relatively small dataset?
    
- Explain your **loss functions** and why they are appropriate for texture/albedo recovery.
    

 

### On fundamentals

Because your CV doesnâ€™t clearly show it, they _will_ test:

- Time/space complexity of common algorithms.
    
- Implementing basic DS (linked list, stack, queue, binary tree) on the whiteboard / online editor.
    
- Graph traversals (BFS/DFS), maybe simple shortest path.
    
- Maybe simple DP problems.
    

Make sure you do **some LeetCode-style practice tonight** (even 5â€“10 problems is better than none) and review:

- Big-O of arrays, vectors, maps, sets.
    
- Common patterns: two-pointer, sliding window, sorting + greedy, BFS/DFS.
    

---

## Priority fixes for _tomorrow_

If you only have limited time before your interview, Iâ€™d do this in order:

1. **Fix glaring issues in the resume**
    
    - Typos (â€œEnvrionmentâ€).
        
    - Remove the template garbage/extra links at bottom.
        
    - Clean showreel password (or remove password).
        
    - Tighten Skills section to be more engineer-focused.
        
2. **Add 1â€“2 sentences per project to show impact + scale**
    
    - J-Renderer: mention C++ version, lines of code (rough), FPS/scene scale, tests or tools.
        
    - Nvdiffrec / Beauty2Albedo: mention dataset size, framework (PyTorch), at least one metric or clear improvement statement.
        
3. **Prepare stories for:**
    
    - One tool/automation you built at Goodbye Kansas.
        
    - One big debugging/optimization story from J-Renderer or Nvdiffrec.
        
    - One example where you took initiative to improve a process.
        
4. **Do a quick algorithms refresh**
    
    - Rehearse verbally how youâ€™d explain BFS/DFS, binary search, hashing.
        
    - Do a couple of easy/medium problems.
        

---





JRenderApp initialize JWindow, JDevice and renderer 

Renderer: 
- create swapchain
- create a vector of JSync:  sync objs
- create a vector of JcommandBuffer



Device:
- create Instance





---
debug
multiple descriptor pool
expectation: Only one descriptor pool created and shared across the whole software
Debug: through gdb debug, 
`break JDescriptorWriter::JDescriptorWriter` then realize there are actually two descriptor pool (been called three times, and one of them , the descriptor pool has different memory address)

---

# Project details





- â€œé¡¹ç›®é‡Œé‡åˆ°æœ€å¤§çš„æŠ€æœ¯æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿâ€
    
- **how would you deal with intermittent hard to reproduce issue?**
	- Check the environment, isolate the environment
	- Check 
	- then Try to reproduce the issue by myself
	- 
    
- â€œä½ å½“æ—¶è€ƒè™‘è¿‡å“ªäº›æ–¹æ¡ˆï¼Ÿä¸ºä»€ä¹ˆé€‰æ‹©äº†æœ€ç»ˆæ–¹æ¡ˆï¼Ÿâ€
    
- â€œå¦‚æœç°åœ¨å†åšä¸€æ¬¡ï¼Œä½ ä¼šæ€ä¹ˆæ”¹è¿›ï¼Ÿâ€
    

ğŸ‘‰ **ç›®çš„ï¼šç†è§£ä½ çš„æ€ç»´æ–¹å¼æ˜¯å¦æˆç†Ÿã€æ˜¯å¦ä¼šç‹¬ç«‹è§£å†³é—®é¢˜ã€‚**

---

## **3. ä½ æ˜¯å¦èƒ½åœ¨ä»–ä»¬çš„å›¢é˜Ÿé‡Œå·¥ä½œå¾—é¡ºç•…ï¼Ÿ**

è¿™åŒ…å«åä½œã€æ²Ÿé€šã€æ€åº¦ã€‚

å› æ­¤ä¼šé—®ï¼š

- What if disagree with teammate (â€œä½ å’Œé˜Ÿå‹æ„è§ä¸ä¸€è‡´æ€ä¹ˆåŠï¼Ÿâ€)
	- I will first make sure I fully understand their perspective, then I will explain my reasoning by using data or existed examples. 
	- If we still disagree, I would suggest that we run a quick test or prototype, and we will check the result together, and find the right decsion.
	- If we need the decision fast, I will ask lead or senior engineer for suggestions or direction.
	- The goal is always solve the problem, and focus on that
    
- â€œæœ‰æ²¡æœ‰é‡åˆ° deadline å¾ˆç´§çš„æƒ…å†µï¼Ÿä½ æ€ä¹ˆåº”å¯¹ï¼Ÿâ€
    
- â€œä½ å’Œ senior engineer çš„åˆä½œæ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿâ€
	- I collaborate with senior engineers by being clear about what I am working on, asking questions early when I'm stuck, and actively seeking feedback on my approach.
	- I try to understand their design choices and learn the reasoning behind them.
	- The goal is build things effectively while learning from their experience.
    
- What kind of engineering culture do you like ? (â€œä½ æœ€å–œæ¬¢æ€æ ·çš„å·¥ç¨‹æ–‡åŒ–ï¼Ÿâ€)
	- I like the culture that  values collaboration, clear communication and learning.
	- I appreciate teams where people give constructive feedback through code reviews, share knowledge openly and help each other grow.
	- I also like document the project and issues that encounter.
    



    

- â€œä½ æœ€è‡ªè±ªçš„é¡¹ç›®æ˜¯ä»€ä¹ˆï¼Ÿâ€
I am very proud of my Vulkan Engine, because of Vulkan API's learning curve is very steep, and I finally manage to build the whole software and able to actually render something. 
    
- â€œä½ é‡åˆ°æœ€å¤±è´¥çš„é¡¹ç›®æ˜¯ä»€ä¹ˆï¼Ÿå­¦åˆ°äº†ä»€ä¹ˆï¼Ÿâ€
The concord game trailer project. It is a huge project, but the outcome is not very satisfying. What I learnt is that, no matter which position I am, I must have strong ownership mindset. In this project, I been 

    
- â€œä½ ä»€ä¹ˆæ—¶å€™æ„è¯†åˆ°è‡ªå·±é”™äº†ï¼Ÿæ€ä¹ˆå¤„ç†çš„ï¼Ÿâ€
When I doing my research project, 
    




# **1. Motivation & Role Fit**

These are almost guaranteed:


    
- **Why do you want to transition from VFX/CG to software engineering?**
I realized what I enjoy in the VFX wasn't artistic side, but I become more and more interested in the technical problem-solving,  I am more interested in how things working under the hood, how the pipeline built, and how the process could be automated.
I was starting to build some small tools and procedural assets for the team, such as the procedural grass, procedural generate organic paper etc. I also helped my colleague to debug render or pipeline specific issues, such as their asset cannot be publish to the pipeline, when change the render engine, the material result doesn't remain the same, etc. I enjoy thinking in systems, and I think that naturally pushed me toward computer science.





- **Tell me about a technical project youâ€™re proud of (e.g., Vulkan engine, Nvdiffrec modification). What were the hardest parts?**
The hardest part for Vulkan engine is the architecture. I want to make a modular system that I can expand in the future, so at the begining after I finished the official tutorial, I was checking the way that make the architecture more modular, I research open-source project, trying to understand other's design reasoning and think about what I can learn from that logic. However since this is a exploring process, there were many times I need to keep changing. For example, at the beginning I write all classes that follows OOP but only cover objects such as material, textures etc. Then after I want to introduce GUI interface, I realize I have to separate the system, area that focus on interactive like ImGUI and camera will be one system, rendering logic should be collect in one etc.
Now after the project is expanding, I realize I will need more, like resource management, scene graph  etc.
    
- **What is the most complex system youâ€™ve built? How did you design it?**
    
- **Whatâ€™s your experience with C++?**  
    They might follow up with:
    
    - memory management
	    - I followed RAII design pattern, the class usually comes with destructor, since Vulkan API 
	    - During the developing, I debugged many memory leaks using vulkan's validation layers, and fixed the issues like I didn't free the vulkan resources in the destructor.
	    - 
        
    - performance optimisations

        
    - threading/concurrency basics
        
- **How comfortable are you with Python?**
    
- **Whatâ€™s the most challenging debugging issue youâ€™ve faced recently?**
    
- **Whatâ€™s your experience with distributed systems or real-time pipelines?**  
    (Even from your VFX USD pipeline work.)
    

---

# **3. Software Engineering Practices**
 
Your VFX background gives you strong teamwork storiesâ€”they will want to test that:

- **Tell me about a time you collaborated under pressure or tight deadlines.**
	
- **How do you handle feedback?**
    
- **Describe a situation where different teams had conflicting requirements. What did you do?**
    
- **Tell me about a miscommunication in a technical project. How did you fix it?**
    

---

**5. Your Experience in VFX and How It Translates**

- **How has your experience with USD, Houdini, or CG pipelines prepared you for systems engineering?**


- **What did you learn from building CG environments that applies to large-scale software?**
They all follow similar development process like agile development. The major difference is that in software engineering, the development could be more flexible.

- **How did you deal with performance bottlenecks in VFX workflows?**





 **6. Culture & Growth Potential**

- **What new technology or topic have you explored recently?**
The new technology I explored recently is the quantum computing from 2025 siggraph. There is a talk about Quantum computing.
    
- **How do you stay up-to-date with engineering best practices?**
I would bookmark the technical blogs (through my life), and follow up those blogs. I also pay attention to the conference such as siggraph, on these conferences I can get to know more new things.
    
- **What are your long-term goals in the field of virtual worlds or platform software?**
As the person who build the platform, I will be the person who also use it, I want to try to target on specific group of people, and I will use the technique to build a project that fit their preferences, also recording the case-specific tutorial to help more people learn how to use it, and show the actual, beautiful, useful result. 
    
- **What makes you stand out from other graduate engineers?**
I have the production-level VFX experience, I know what users want.
    

---

# **7. Specific Questions They'll Likely Ask _You_ Given Your CV**

These are tailored to your actual experience:

- **Your Vulkan engine project: what architectural decisions did you make and why?**  
    (They will dig into thisâ€”it shows systems-level thinking.)
    
- **You modified Nvdiffrec: what problem were you solving, and what did you change in the optimisation loop?**
    
- **How did you manage team coordination at Goodbye Kansas when delivering complex environments?**
    
- **What is one technical achievement from your VFX career that demonstrates engineering ability?**
    
- **How do you compare building CG pipelines vs. software systems?**
    

---

# **8. Questions They May Ask Toward the End**

These are typical closing questions:

- **What do you expect from your mentor or team?**


- **How do you like to receive feedback?**
I would like constructive feedback, it would be even better if there are specific examples that can be referenced.
    

---
5## Questions
1. What kind of candidates would be the best for this role, what kind of people doing great on this role.
2. What is the career path, like people you see are doing what directions
3. What are some of the most interesting and impressive career paths you have seen from people who started in this position?
4. would you like to tell me about the structure about the team and whom I will be working with 
5. I am wondering if this role is new headcount or backfill?




**What is your understanding of a technical assistant? Do you know what does Technical assistant do?**
- what I will do is support artists when they meet technical difficulties, set up anything that artists need to do their magic. like loading plates, restoring the old work, archive all works.
- Monitoring and manage the render farm, re-piorities the renders. 
- Sometimes build automation tools to improve the efficiency of the pipeline
- As I understand, this is an entry level job for the technical role of visual effects industry


**Introduce:**
Junyi -> start career as CG Generalist -> previously worked in Goodbye Kansas studios -> participated in films, tv series, cinematic game trailer -> after years working -> being artist doesn't really match what I want to do -> 
I am more interested in tech, how to automate stuff, how everything works under the hood, why my render is slow, can I make it better? what is the latest trend of machine learning and AI stuff that happen to visual effects, can those things make vfx more productive?
-> back to uni learn cs -> I learned more advanced math, learn from statistics to all components of stable diffusion model -> fine tune stable diffusion from code, from here I start able to read paper 
->  modify nvdiffrec framework understand differentiable rendering, actually bring in my own ideas and trying to build solutions ->
build my own vulkan engine using C++, on the one hand understand how does rendering work, what can make the render slow, on the other hand this gives me a solid software development experience, I am able to write unit test,  debug, profiling to see what is strange, using Jira to track the progress etc
-> I also use python build a HDRI browser in Houdini recently, this is not shown on the cv that you have now. Build in HDRI browser in Houdini, artist can directly view HDRI and import them houdini directly with one click or drag and drop.
-> I really enjoy the technical side of visual effects, and I saw this position post, and I feel I could be a good fit for this role. Since I want to develop technical skills in VFX direction, ILM is the top company in this area. Also, since I was an artist previously, I am very familiar with the visual effects pipeline and I fully understand what artist want, what do they care,




**How do you see yourself in 5 years**

In general, I would want to focus on the technical solution for VFX industry, I will be expert on the technical details of vfx production. 
I think I will specialize in the pipeline and also do some research work, because since I do nvdiffrec project, I feel if I can refine this method and make it a useful tool, we can create digital double more accurately, because it output the separate texture layers.

If we breakdown, 
In next one or two years, I would see myself mastering the core responsibility of the technical assistant role, and I would be a reliable person that can troubleshoot complex issues across different department. 

After that, maybe I will shift to the pipeline team and build the production tools.

Grow deep in technical,

1.  I would see myself as an top expert focus on technical solution in VFX area, the potential career path would become a pipeline td, this could change because the techniques nowadays is developing extremely fast. I am very open to any opportunites as long as it is the tech side of this industry.
2. Develop pipeline for the specific show, fix technical issues.
3. Be more practical, I aiming to spot the issues that vfx artist or vfx studios have, and write tools or software to help them, or integrate the latest techniques. or even possible I wish I can publish paper.  
4. I will see myself still a strong learner after 5 years,  I will stay open-minded and continue embracing new techniques.


**Talk about your project**

**What tool you have done**
During my time in Goodbye kansas studios, I did many HDA procedural assets, for example the procedural pipes, procedural planet, organic assets
I also write some simple vex script to control normal or tangent direction. Simple python script for saving nodes, and recreate them.
After I practice the programming skills, I did bigger project, several weeks ago I made a HDRI browser in the Houdini

**What is your weakness:**
Sometimes I get very drawn into technical details, and spend too long time on it. I've become much more aware of this, so I have started structuring my work more deliberately. I break tasks down in  Jira  and use sprint style approach to keep myself focused on milestones. If I find something that I want to investigate further, I tag it and schedule it for later. 


Collection of other peoples jd:
- Troubleshooting and dynamic problem solving for layout, animation, lighting and post-production workflows.  
- Image/Media handling, including conversion, manipulation, quality control and playback for visual reviews.  
- Render troubleshooting and queue management.

- Data management and render wrangling/support for shows in production which involved:  
a. Overseeing incoming/outgoing data to and from the studio  
b. Ensuring the smooth and correct ingestion of plates through the pipeline, as well as  
supporting the process technically  
c. Making decisions to ensure the best utilization of the render farm  
d. Collecting overnight render stats and compiling morning reports to communicate back to the Senior Producers  
- Hands-on troubleshooting shot issues with in-house tools and workflows/processes, requiring continuous communication with artists and Production  
- Responsible for maintaining and further developing pipeline/department scripts/tools, as well as updating the showâ€™s pipeline setup, along with the documentation needed for those tools

(level up)
- Started taking over more challenging pipeline tasks  
- Frequent communication with CG and Comp Supervisors  
- Became a member of the show setup team:  
a. Interpret and implement on the pipeline the clientâ€™s requests on the provided Technical Spec Sheet  
b. Better understanding of image processing/science and colour  
- More managerial responsibilities within the department - acting for the Data Manager in their absence  
- Involved in the forecasting of the rendering and data activity for the shows in production at the beginning of each week that involved making important decisions  
- Coordinating the archiving of a show once completed  
- Became a member of the interviewing panel for new hires, and was part of the decision making process that followed up each interview. Then offered daily and continuous training and assistance for new hires on the in-house tools and workflows



git merge:
git checkout main : go to the main branch
git merge feature: merge the feature branch into the main  branch




 **2. Linux/Unix èƒ½åŠ›**

80% çš„æŠ€æœ¯é—®é¢˜éƒ½è·Ÿ Linux æ–‡ä»¶ç³»ç»Ÿã€æƒé™ã€ç¯å¢ƒå˜é‡ç›¸å…³ã€‚å¸¸è§é¢è¯•é¢˜ï¼š
- å¦‚ä½•ç”¨å‘½ä»¤æ‰¾å‡ºä¸€ä¸ªç›®å½•å ç”¨ç£ç›˜æœ€å¤šçš„æ–‡ä»¶ï¼Ÿ
- å¦‚ä½•æŸ¥çœ‹ä¸€ä¸ªè¿›ç¨‹çš„ CPU/RAMï¼Ÿ
- å¦‚ä½•åˆ¤æ–­ç½‘ç»œè¿é€šæ€§ï¼ˆå¦‚æ¸²æŸ“èŠ‚ç‚¹æ— æ³•è®¿é—®å­˜å‚¨ï¼‰ï¼Ÿ
- å¦‚ä½•æŸ¥çœ‹ä¸€ä¸ª Python è„šæœ¬ä¸ºä»€ä¹ˆè·‘ä¸èµ·æ¥ï¼Ÿ
    

---

 **3. Python/Shell è„šæœ¬ & è‡ªåŠ¨åŒ–**

TA è¦ç»å¸¸å†™å°è„šæœ¬è§£å†³é—®é¢˜ï¼Œå› æ­¤æµ‹è¯•ï¼š
- Python åŸºç¡€è¯­æ³•
- æ€æ ·è¯»å–ã€å†™å…¥å¤§é‡æ–‡ä»¶
- æ‰¹é‡æ–‡ä»¶é‡å‘½åè„šæœ¬
- è§£æ logã€æœé›†ç»Ÿè®¡æ•°æ®
- å¦‚ä½•å®šæœŸæ£€æŸ¥ç£ç›˜ã€è‡ªåŠ¨æ¸…ç†ç¼“å­˜


    
 **5. å›¾åƒå¤„ç† & è§†é¢‘åŸºç¡€çŸ¥è¯†**

å› ä¸º JD æåˆ° image/media handlingï¼Œå¸¸å‡ºç°ï¼š
- EXRã€MOVã€DPX çš„åŒºåˆ«ï¼Ÿ
	- exr is image sequence, linear, 16-bit or 32-bit, multiple compress method, multiple channels/layers
	- dpx: also image sequence, for color grading and storage (store the **scanning from films**), 10bit or 16 bit, usually log and uncompressed
	- mov: contained, rely on codec, 8-bit to 12/16-bit
    
- è‰²å½©ç©ºé—´ï¼ˆsRGBã€ACESï¼‰çš„åŸºæœ¬æ¦‚å¿µ
	- SRGB, the most common colorspace, relatively small color gamut, use gamma2.2, so allow images show on LDR monitor.
		- Cons: cannot save high saturation and high luminance information.
	- ACES: Academy color encoding system. Big color gamut, use 16-bit or 32-bit. You need to understand what colorspace is your file, so you can view it correctly
	- OCIO: (OpenColorIO): load aces or curstomized configurations, make sure all software and artists are working under the same settings
	- LUT: viewing transform. Allow users are seeing the correct output, but data itself not change
	- VFX always follow linear workflow. All filmed data need to convert to linear colorspace
	- CG renders need to be set as linear(exr)
	  
    
- å¸§ç‡é—®é¢˜
    
- ç”¨ä»€ä¹ˆå·¥å…·æ£€æŸ¥ metadataï¼ˆOIIOã€ffmpegï¼‰
-

**OpenImageIO:** 

`iinfo -v xxx.exr`: Get general information of exr
> channels, data format, software



The reason to check is for Quality control(QC)
- **Color integrity**: for files are being marked as Log is not mistakenly treated as Linear.
- **Continuity**: verify that timecodes and frame numbering are sequential and match editorial logs, preventing expensive reshoots or re-render
- **Debug/Troubleshooting**: if shot looks wrong, maybe can find something in here




Use the `ffprobe` utility, which is included with FFmpeg, with the `-show_format` and `-show_streams` options.

`ffprobe -v error -select_streams v:0 -show_entries stream=codec_name,width,height,avg_frame_rate:format=duration,bit_rate,tags -of default=noprint_wrappers=1 input_video.mov`

| **Output Section**     | **Data You Check**                        | **Why You Check It**                                                                 |
| ---------------------- | ----------------------------------------- | ------------------------------------------------------------------------------------ |
| **`codec_name`**       | ProRes, H.264, etc.                       | Ensures the correct codec was used for delivery.                                     |
| **`avg_frame_rate`**   | $24/1$ (24 fps), $30000/1001$ (29.97 fps) | Verifies timing is correct and consistent with project standards.                    |
| **`tags:timecode`**    | Timecode value.                           | Essential for syncing video with editorial and audio tracks.                         |
| **`tags:color_space`** | Rec.709, BT.2020, etc.                    | Provides a high-level check, but often **not** the deep color data (see OIIO below). |


**Imagine artist report the local workstation not working same as renderfarm**
1. confirm that I receive this report and I am starting the investigation
2. Ask for the file (local workstation) and job ID on the farm, exactly which frame is failed
3. **env:** The mismatch of local software version and farm software version? 
4. **linking issue:** Path different? (maybe not the same network mount, absolute or relative path )  license issue?
5. manually launch a small, single frame test job under the same submission script, if work, linking/data-specific issue, if not work, env issue

**env:**
- File permission/mounting
- plugin missing?
- system difference?

**data-specific**:
- data corruption?
- if it is frame specific issue, I will isolate those frames and see what unique elements are there

```bash
rsync -av source/ dest/
```



âœ” tail log æŸ¥æ¸²æŸ“è¿›åº¦

```bash
tail -f /var/log/render.log
```





---

 â–¶ **æƒé™ç›¸å…³**

```bash
chmod
chown
chgrp
stat
umask
```

å¾ˆå¤šè‰ºæœ¯å®¶æ— æ³•å†™å…¥ã€æ¸²æŸ“å¤±è´¥å°±æ˜¯æƒé™é—®é¢˜ã€‚

---

 â–¶ **è¿›ç¨‹/æ€§èƒ½æ’æŸ¥**

```bash
top
htop
ps aux
kill -9
nice
renice
```

ç”¨äºï¼š
- æŸ¥çœ‹ Mayaã€Nuke å¡æ­»
    - æ¸²æŸ“èŠ‚ç‚¹ CPU è´Ÿè½½æ˜¯å¦æ­£å¸¸
    - åˆ¤æ–­æ˜¯å¦å› å†…å­˜ä¸è¶³å¯¼è‡´å¤±è´¥
    

 â–¶ **ç½‘ç»œå’ŒæŒ‚è½½é—®é¢˜**

```bash
ping
traceroute
mount
umount
df -h | grep nfs
```

ç”¨äºï¼š
- æ¸²æŸ“èŠ‚ç‚¹æ— æ³•è®¿é—®ç´ æ
- NFS å­˜å‚¨æ‰çº¿
- æ£€æŸ¥æŒ‚è½½ç‚¹æ˜¯å¦æŒ‚æ­»
    
 â–¶ **ç¯å¢ƒå˜é‡ï¼ˆPipeline æ ¸å¿ƒï¼‰**

```bash
printenv
env
export
source
which maya
```

Pipeline å‡ºé—®é¢˜ï¼Œæœ‰ä¸€åŠæ˜¯ **ç¯å¢ƒå˜é‡é”™äº†**ã€‚

Check env path:
`echo $PATH`

Add new path to PATH:
`nano ~/.bashrc` 
`export PATH=$PATH:/opt/mytools/bin`  (if only do  this in the terminal, it is temporary)
`source ~/.bashrc` reload your shell (added new path permanently)




```bash
tail -f /path/to/log
grep -R "error" .
less
```

---
- `oiiotool`ï¼ˆOpenImageIOï¼ŒEXR/å›¾åƒå¤„ç†ï¼‰
- `ffmpeg`ï¼ˆè§†é¢‘å·¥å…·ï¼‰

- â€œå¦‚ä½•ç”¨å‘½ä»¤è¡ŒæŠŠ EXR è½¬æˆ JPGï¼Ÿâ€   
`oiiotool input.exr -o output.jpg`
`oiiotool input.exr --quality 95 -o output.jpg`
`oiiotool *.exr -o %04d.jpg`

    




**Why you transfer from art to tech**
For the long time, I realize everytime when I approaching an issue, my brain is wired for logic and system

While working as an artist, I realized that my approach to creating shots was a bit different from others. I always want to find a way to build the system or build 
therefore I like building the procedural asset or tools
 I am always interested in things under the hood
 

For example, we started a project, and start with a very simple task, pipe. 


basically some kind of thinking like this, but it actually not sound verynice, it can give a vibe that I am not good with my old job so I want to try something new. no emplyer want to hear that

I am still in vfx industry, just change from artist job to technical side of vfx job, like technical assistant pipeline td 


What is your motivation for this role


**What is your motivation for ILM**
**Why do you want to join ILM**

I want to join because ILM producing high-end visual effects, and this is where hardest problems are, and I would be very happy to challenge myself and solve difficult problems.

Also, with Disney Research behind it I think ILM is the place where new techniques are implemented, I want to see first hand how these advanced technologies are integrated into the production.



**Why do you think you fit**
1. I was an artist, I have the real production experience that I can see issues in the shot
2.  I am not afraid of difficulties and I welcome challenges. Harder tasks usually come with greater rewards 
3. I consider myself very strong self-study ability, I always make notes, I have my own notebook system, 
4. I have strong passion for visual effects



**Do self introduction again:**
Except the part that we have discussed last time, I think maybe this time we can go deeper into the 



Elaborate more on the technical side:


**The most difficult task in the previous work:**
In a previous project, I need to create a big corn field, the camera was first follow the main character, and then gradually increase the height to see the whole scene. So first we found some detailed corn model, and I procedural modeled leaves and the stem, and put back them together, so I can have many different variant of individual corns, then I did the similar thing on the weed on the root,. Also, consider that naturally, each corn has different spread of its weed on the ground, and they usually keep distance from each other, the simple scattering corn positions will not work in this case.  I wrote a vex script that delete overlap script under the certain threshold.   Also this spread attribute also been used on the ground texturing, so I use this spread as weight to mix several different soil type, to introduce more realism. Everything is done in Houdini, so all materials can introduced with some randomness based on the unique id, like this corn and its weed are a more yellow than others, and also layered with randomness on the  



**How do you see yourself in 5 years**
**What do you want to do in the future**

1. In the first one or two years, I want to fully understand ILM's workflows, and be excellent on this role,
	1. I would be very comfortable communicate and understand artist, writing the tool that fix their issues. Currently I am developing more tools that I found could be useful if I am an artist, and 
2. Then for the long term, I want to be expert in technical solution focus on the VFX, focus on the practical and efficient solutions. Through development I want to handle bigger and complex task. If ILM going to implement new technology, I will be able to help to integrate that into the pipeline.
3. In my free time, I am going to continue studying computer graphics fundamc entals, I feel this could potentially help me in some complex, low-level crashes or bugs. I wanted to build deep technical understanding in this area.     computer graphic.



I really enjoy the feeling that, in a project, or in a production, I meet an issue, I will target on this issue and find ways to solve it, keep thinking about it, maintain and improve it. 




**Help other people:**

In my previous work in GBK, we were integrating into the new USD pipeline, our HOD was building the tool, so we import alembic file, and go through the tool, it will have the correct name, LOD, showing correct in the Solaris, 
One of my colleague was asking me for help, she imported the alembic file, but after switching to LOP, there is nothing in the viewport, so we were having the call, share the screen and investigate it together, I open the tool and dive inside, I deleted all unnecessary attributes in the SOP first, then I went through all nodes in the tool in LOP one by one, checking the data flow, 
I remember the reason is that the tool was developing in the early stage, and there is an edge case that the tool didn't consider, and when pack the geometry, everything become points, and in the LOP viewport, also because the tool by default only show the lowest LOD, so there is only one point left, and make it looks like there is nothing in the viewport



**Questions:**

will there be separate pool for different show

How do you define priority
How do yo define which data going to be deleted

Tracking dependecies of each show

Do you have a system that track the heavy files on a on

Will you delete heavy cache on the ongoing show time to time, because of tracking the dependecies, or tags,
or you will not regularly delete things untill the storage is almost full, then find the biggest file, contact with artist to confirm if this could be deleted



How many people in the team
My responsibility will be solely on London or also need to care about other sites
Are you using any cloud, how much 
.


